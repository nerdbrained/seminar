<!DOCTYPE html>
<html lang="en"><head><meta charset="utf-8"/><meta name="google-site-verification" content="NaytRH7PCHM8SH9XV-xgMLEi_1m1wS9lPBAIvTJ-wvs"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="//wumss.github.io/rabbit/rabbit.min.css"/><link rel="stylesheet" href="/css/custom.css"/><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-93527069-1', 'auto');
ga('send', 'pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    jax: [
        "input/TeX",
        "input/MathML",
        "input/AsciiMath",
        "output/CommonHTML"
    ],
    extensions: [
        "tex2jax.js",
        "mml2jax.js",
        "asciimath2jax.js",
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js"
    ],
    TeX: {
        extensions: [
            "AMSmath.js",
            "AMSsymbols.js",
            "noErrors.js",
            "noUndefined.js"
        ]
    },
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": {
        availableFonts: ["TeX"]
    }
});
</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script><title>Information theory</title></head><body><header><nav><input id="nav-trigger" class="nav-trigger" type="checkbox" checked="checked"/><ul><li><a href="/"><img src="/images/logo.png" height="64" width="64" alt="Home" title="Home"/></a></li><li><a href="/archive/">archive</a></li><li><a href="/tag/">tags</a></li><li><a href="/faq/">faq</a></li><li><a href="/document/">library</a></li><li><a href="/potential-topics/">topics</a></li></ul><label for="nav-trigger"></label></nav></header><main><article><h1>Information theory</h1><p>There have been 1 completed talk and 2 topic suggestions tagged with <b>information theory</b>.</p><h2>Related Tags</h2><ul><li><a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a></li><li><a class="tag-link tag-kolmogorov-complexity" href="/tag/kolmogorov-complexity/">Kolmogorov complexity</a></li><li><a class="tag-link tag-signal-processing" href="/tag/signal-processing/">signal processing</a></li><li><a class="tag-link tag-coding-theory" href="/tag/coding-theory/">coding theory</a></li><li><a class="tag-link tag-probability" href="/tag/probability/">probability</a></li><li><a class="tag-link tag-theoretical-computer-science" href="/tag/theoretical-computer-science/">theoretical computer science</a></li><li><a class="tag-link tag-linear-algebra" href="/tag/linear-algebra/">linear algebra</a></li><li><a class="tag-link tag-algebra" href="/tag/algebra/">algebra</a></li></ul><h2>Completed Talks</h2><h3><a href="/archive/ss-Information">Information Theory</a></h3><p>Delivered by Sidhant Saraogi on Friday October 14, 2016</p><p>I will try to provide a brief introduction to Information Theory working towards motivating Shannon&#39;s Source Coding Theorem. We will use rather simple examples (for e.g. Repetition Codes) to explain the idea of noisy channels and  similarly simple examples to explain the idea behind the theorem and  eventually try to prove it for a rather specific example. (if we have the time !)</p><h2 id="suggestions">Talk Suggestions</h2><h3>Compressed Sensing</h3><div><p>Compressed sensing is about minimizing the information gathered and stored by sensors, reducing the need for file compression later on for transmission. This can reduce costs for certain applications, such as non-visible wavelength cameras.</p></div><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="http://www.ams.org/samplings/math-history/hap7-pixel.pdf">Compressed Sensing Makes Every Pixel Count</a></p></div></li><li><div><p><a href="http://statweb.stanford.edu/~markad/publications/ddek-chapter1-2011.pdf">Davenport, M. A., Duarte, M. F., Eldar, Y. C., &amp; Kutyniok, G. (2011). Introduction to compressed sensing. Preprint, 93(1), 2.</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Compressed Sensing" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Compressed Sensing" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-algebra" href="/tag/algebra/">algebra</a> <a class="tag-link tag-information-theory" href="/tag/information-theory/">information theory</a> <a class="tag-link tag-linear-algebra" href="/tag/linear-algebra/">linear algebra</a> <a class="tag-link tag-signal-processing" href="/tag/signal-processing/">signal processing</a> <a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a> </p><h3>Entropy in Mathematics and Information Theory</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://ee.stanford.edu/~gray/it.pdf">Robert M. Gray</a></p></div></li><li><div><p><a href="http://astarte.csustan.edu/~tom/SFI-CSSS/info-theory/info-lec.pdf">Tom Carter</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Entropy in Mathematics and Information Theory" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Entropy in Mathematics and Information Theory" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-information-theory" href="/tag/information-theory/">information theory</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> <a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a> </p></article><footer><p>Help improve this page by <a href="https://github.com/wumss/seminar/edit/master/wiki/tag/information-theory.md">editing it on GitHub</a>.</p></footer></main></body></html>
