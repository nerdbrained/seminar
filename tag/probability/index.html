<!DOCTYPE html>
<html lang="en"><head><meta charset="utf-8"/><meta name="google-site-verification" content="NaytRH7PCHM8SH9XV-xgMLEi_1m1wS9lPBAIvTJ-wvs"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="//wumss.github.io/tacit/tacit.min.css"/><link rel="stylesheet" href="/css/custom.css"/><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-93527069-1', 'auto');
ga('send', 'pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    jax: [
        "input/TeX",
        "input/MathML",
        "input/AsciiMath",
        "output/CommonHTML"
    ],
    extensions: [
        "tex2jax.js",
        "mml2jax.js",
        "asciimath2jax.js",
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js"
    ],
    TeX: {
        extensions: [
            "AMSmath.js",
            "AMSsymbols.js",
            "noErrors.js",
            "noUndefined.js"
        ]
    },
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": {
        availableFonts: ["TeX"]
    }
});
</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script><title>Probability</title></head><body><header><nav><input id="nav-trigger" class="nav-trigger" type="checkbox" checked="checked"/><ul><li><a href="/"><img src="/images/logo.png" height="64" width="64" alt="Home" title="Home"/></a></li><li><a href="/archive/">archive</a></li><li><a href="/tags/">tags</a></li><li><a href="/faq/">faq</a></li><li><a href="/document/">library</a></li><li><a href="/potential-topics/">topics</a></li></ul><label for="nav-trigger"></label></nav></header><main><article><h1>Probability</h1><p>There have been 4 completed talks and 5 topic suggestions tagged with <b>probability</b>.</p><h2>Related Tags</h2><ul><li><a class="tag-link tag-algorithms" href="/tag/algorithms/">algorithms</a></li><li><a class="tag-link tag-space-efficiency" href="/tag/space-efficiency/">space efficiency</a></li><li><a class="tag-link tag-data-structure" href="/tag/data-structure/">data structure</a></li><li><a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a></li><li><a class="tag-link tag-information-theory" href="/tag/information-theory/">information theory</a></li><li><a class="tag-link tag-combinatorics" href="/tag/combinatorics/">combinatorics</a></li><li><a class="tag-link tag-algorithm" href="/tag/algorithm/">algorithm</a></li><li><a class="tag-link tag-decision-theory" href="/tag/decision-theory/">decision theory</a></li></ul><h2>Completed Talks</h2><h3><a href="/archive/fr-MEDR">Metric embeddings and dimensionality reduction</a></h3><p>Delivered by Frieda Rong on Friday March 31, 2017</p><p>In this talk, we consider embeddings which preserve the pairwise distances of a set of points.  It is often useful to find mappings from one high dimensional space to a lower dimensional space that preserve the geometry of the points. One source of applications is in streaming large amounts of data, for which storage is costly and/or impractical. However, the study of such embeddings has also inspired developments in the design of approximation algorithms and compressed sensing.</p><p>At the crux of the talk is the remarkable Johnson-Lindenstrauss lemma. This fundamental result shows that for Euclidean spaces, it is possible to achieve significant dimensionality reduction of a set of points while approximately preserving the pairwise distances. An elementary proof will be given, along  with subsequent speed improvements with sparse projections and an interesting use of the Fourier transform. We will also discuss applications of the lemma to the fields mentioned above.</p><h3><a href="/archive/lm-BF">Bloom Filters and Other Probabilistic Data Structures</a></h3><p>Delivered by Luthfi Mawarid on Friday March 3, 2017</p><p>With the advent of big data, the ability to process large volumes of data is becoming increasingly important. For instance, when dealing with large data sets, we may want to perform simple operations such as counting the number of unique elements or checking whether or not an element is present in the set. While there are deterministic data structures, such as hash tables, that can perform these quickly, the sheer size of the data involved makes their use largely  impractical and unscalable.Instead, we may want to trade-off some accuracy in our  answers in exchange for greater space efficiency and ease of parallelization.  For this, we introduce the concept of probabilistic data structures. </p><p>In this talk, we will mainly focus on Bloom filters, which are commonly used to test set membership and speed up data access. We will explore its main use cases, its implementation details, and the mathematics behind it. If time permits, I will also talk about the count min-sketch, used for frequency counting, and/or the HyperLogLog counter, used for cardinality estimation.</p><p>This talk will assume basic knowledge of probability.</p><h3><a href="/archive/fr-RandomGraphs">Random Graphs and Complex Networks</a></h3><p>Delivered by Frieda Rong on Friday November 11, 2016</p><p>From the graph of Facebook friendships to the neurons inside your brain, networks are all around us. Weâ€™ll go over some surprising <em>connections</em> in network theory and see some of the following:</p><h3><a href="/archive/ss-Information">Information Theory</a></h3><p>Delivered by Sidhant Saraogi on Friday October 14, 2016</p><p>I will try to provide a brief introduction to Information Theory working towards motivating Shannon&#39;s Source Coding Theorem. We will use rather simple examples (for e.g. Repetition Codes) to explain the idea of noisy channels and  similarly simple examples to explain the idea behind the theorem and  eventually try to prove it for a rather specific example. (if we have the time !)</p><h2 id="suggestions">Talk Suggestions</h2><h3>Bloom Filters and Probabilistic Data Structures</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.cs.dal.ca/sites/default/files/technical_reports/CS-2002-10.pdf">Blustein</a></p></div></li></ul><p>Past and scheduled talks on a related subject include</p><ul><li><span><a href="/archive/lm-BF">Bloom Filters and Other Probabilistic Data Structures</a> by Luthfi Mawarid</span></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Bloom Filters and Probabilistic Data Structures" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Bloom Filters and Probabilistic Data Structures" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-algorithms" href="/tag/algorithms/">algorithms</a> <a class="tag-link tag-computer-science" href="/tag/computer-science/">computer science</a> <a class="tag-link tag-data-structure" href="/tag/data-structure/">data structure</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> <a class="tag-link tag-space-efficiency" href="/tag/space-efficiency/">space efficiency</a> </p><h3>Entropy in Mathematics and Information Theory</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://ee.stanford.edu/~gray/it.pdf">Robert M. Gray</a></p></div></li><li><div><p><a href="http://astarte.csustan.edu/~tom/SFI-CSSS/info-theory/info-lec.pdf">Tom Carter</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Entropy in Mathematics and Information Theory" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Entropy in Mathematics and Information Theory" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-information-theory" href="/tag/information-theory/">information theory</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> <a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a> </p><h3>Gillespie Algorithm</h3><div><p>The Gillespie Algorithm for stochastic equations is used heavily in a number of fields of applied mathematics, in particular computational systems biology.</p></div><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.cs.princeton.edu/picasso/seminarsS05/Karig_slides.pdf">David Karig</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Gillespie Algorithm" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Gillespie Algorithm" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-algorithms" href="/tag/algorithms/">algorithms</a> <a class="tag-link tag-applied-mathematics" href="/tag/applied-mathematics/">applied mathematics</a> <a class="tag-link tag-computational-mathematics" href="/tag/computational-mathematics/">computational mathematics</a> <a class="tag-link tag-computer-science" href="/tag/computer-science/">computer science</a> <a class="tag-link tag-differential-equation" href="/tag/differential-equation/">differential equation</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> <a class="tag-link tag-stochastic-equation" href="/tag/stochastic-equation/">stochastic equation</a> </p><h3>Optimal Stopping Theory and the Secretary Problem</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.math.upenn.edu/~ted/210F10/References/Secretary.pdf">Thomas S. Ferguson</a></p></div></li><li><div><p><a href="http://theory.stanford.edu/~sergei/slides/hiring-dagstuhl.pdf">Sergei Vassilvitski</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Optimal Stopping Theory and the Secretary Problem" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Optimal Stopping Theory and the Secretary Problem" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-decision-theory" href="/tag/decision-theory/">decision theory</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> <a class="tag-link tag-statistics" href="/tag/statistics/">statistics</a> </p><h3>Random Walks and Self-Avoiding Walks</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="http://www.ms.unimelb.edu.au/~guttmann@unimelb/lectures/MAV2003.pdf">Tony Guttmann</a></p></div></li><li><div><p><a href="https://www.math.ubc.ca/~slade/spa_proceedings.pdf">Gordon Slade</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Random Walks and Self-Avoiding Walks" rel="nofollow">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Random Walks and Self-Avoiding Walks" rel="nofollow">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a class="tag-link tag-combinatorics" href="/tag/combinatorics/">combinatorics</a> <a class="tag-link tag-probability" href="/tag/probability/">probability</a> </p></article><footer><p>Help improve this page by <a href="https://github.com/wumss/seminar/edit/master/wiki/tag/Remarkable.Tags.Tag(&quot;probability&quot;).md">editing it on GitHub</a></p>.</footer></main></body></html>
