<!DOCTYPE html>
<html><head><meta charset="utf-8" /><meta content="NaytRH7PCHM8SH9XV-xgMLEi_1m1wS9lPBAIvTJ-wvs" name="google-site-verification" /><link rel="stylesheet" href="//yegor256.github.io/tacit/tacit.min.css" /><link rel="stylesheet" href="/css/custom.css" /><script type="text/x-mathjax-config">MathJax.Hub.Config({
    jax: [
        "input/TeX",
        "input/MathML",
        "input/AsciiMath",
        "output/CommonHTML"
    ],
    extensions: [
        "tex2jax.js",
        "mml2jax.js",
        "asciimath2jax.js",
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js"
    ],
    TeX: {
        extensions: [
            "AMSmath.js",
            "AMSsymbols.js",
            "noErrors.js",
            "noUndefined.js"
        ]
    },
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": {
        availableFonts: ["TeX"]
    }
});
</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script><title>Tag Remarkable.Tags.Tag("probability")</title></head><body><nav><ul><li><a href="/"><img src="/images/logo.png" width="64" alt="Home" title="Home" height="64" /></a></li><li><a href="/archive">archive</a></li><li><a href="/tags">tags</a></li><li><a href="/faq">faq</a></li><li><a href="/document">library</a></li><li><a href="/potential-topics">topics</a></li><li><a href="/submit-talk">speak</a></li></ul></nav><section><article><h1>Probability</h1><p>There have been 2 completed talks, 1 scheduled talk, and 5 topic suggestions tagged with <b>probability</b>.</p><h2>Related Tags</h2><ul><li><a href="/tag/algorithms" class="tag-link tag-algorithms">algorithms</a></li><li><a href="/tag/space-efficiency" class="tag-link tag-space-efficiency">space efficiency</a></li><li><a href="/tag/statistics" class="tag-link tag-statistics">statistics</a></li><li><a href="/tag/data-structure" class="tag-link tag-data-structure">data structure</a></li><li><a href="/tag/information-theory" class="tag-link tag-information-theory">information theory</a></li><li><a href="/tag/decision-theory" class="tag-link tag-decision-theory">decision theory</a></li><li><a href="/tag/networks" class="tag-link tag-networks">networks</a></li><li><a href="/tag/stochastic-equation" class="tag-link tag-stochastic-equation">stochastic equation</a></li></ul><h2>Completed Talks</h2><h3><a href="/archive/fr-RandomGraphs">Random Graphs and Complex Networks</a></h3><p>Delivered by Frieda Rong on Friday November 11, 2016</p><p>From the graph of Facebook friendships to the neurons inside your brain, networks are all around us. Weâ€™ll go over some surprising <em>connections</em> in network theory and see some of the following:</p><h3><a href="/archive/ss-Information">Information Theory</a></h3><p>Delivered by Sidhant Saraogi on Friday October 14, 2016</p><p>I will try to provide a brief introduction to Information Theory working towards motivating Shannon's Source Coding Theorem. We will use rather simple examples (for e.g. Repetition Codes) to explain the idea of noisy channels and  similarly simple examples to explain the idea behind the theorem and  eventually try to prove it for a rather specific example. (if we have the time !)</p><h2>Scheduled Talks</h2><h3><a href="/archive/lm-BF">Bloom Filters and Other Probabilistic Data Structures</a></h3><p>Delivered by Luthfi Mawarid on Friday March 3, 2017</p><p>With the advent of big data, the ability to process large volumes of data is becoming increasingly important. For instance, when dealing with large data sets, we may want to perform simple operations such as counting the number of unique elements or checking whether or not an element is present in the set. While there are deterministic data structures, such as hash tables, that can perform these quickly, the sheer size of the data involved makes their use largely  impractical and unscalable.Instead, we may want to trade-off some accuracy in our  answers in exchange for greater space efficiency and ease of parallelization.  For this, we introduce the concept of probabilistic data structures. </p><p>In this talk, we will mainly focus on Bloom filters, which are commonly used to test set membership and speed up data access. We will explore its main use cases, its implementation details, and the mathematics behind it. If time permits, I will also talk about the count min-sketch, used for frequency counting, and/or the HyperLogLog counter, used for cardinality estimation.</p><p>This talk will assume basic knowledge of probability.</p><h2 id="suggestions">Talk Suggestions</h2><h3>Bloom Filters and Probabilistic Data Structures</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.cs.dal.ca/sites/default/files/technical_reports/CS-2002-10.pdf">Blustein</a></p></div></li></ul><p>Past and scheduled talks on a related subject include</p><ul><li><span><a href="/archive/lm-BF">Bloom Filters and Other Probabilistic Data Structures</a> by Luthfi Mawarid</span></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Bloom Filters and Probabilistic Data Structures">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Bloom Filters and Probabilistic Data Structures">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a href="/tag/algorithms" class="tag-link tag-algorithms">algorithms</a> <a href="/tag/computer-science" class="tag-link tag-computer-science">computer science</a> <a href="/tag/data-structure" class="tag-link tag-data-structure">data structure</a> <a href="/tag/probability" class="tag-link tag-probability">probability</a> <a href="/tag/space-efficiency" class="tag-link tag-space-efficiency">space efficiency</a> </p><h3>Entropy in Mathematics and Information Theory</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://ee.stanford.edu/~gray/it.pdf">Robert M. Gray</a></p></div></li><li><div><p><a href="http://astarte.csustan.edu/~tom/SFI-CSSS/info-theory/info-lec.pdf">Tom Carter</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Entropy in Mathematics and Information Theory">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Entropy in Mathematics and Information Theory">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a href="/tag/information-theory" class="tag-link tag-information-theory">information theory</a> <a href="/tag/probability" class="tag-link tag-probability">probability</a> <a href="/tag/statistics" class="tag-link tag-statistics">statistics</a> </p><h3>Gillespie Algorithm</h3><div><p>The Gillespie Algorithm for stochastic equations is used heavily in a number of fields of applied mathematics, in particular computational systems biology.</p></div><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.cs.princeton.edu/picasso/seminarsS05/Karig_slides.pdf">David Karig</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Gillespie Algorithm">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Gillespie Algorithm">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a href="/tag/algorithms" class="tag-link tag-algorithms">algorithms</a> <a href="/tag/applied-mathematics" class="tag-link tag-applied-mathematics">applied mathematics</a> <a href="/tag/computational-mathematics" class="tag-link tag-computational-mathematics">computational mathematics</a> <a href="/tag/computer-science" class="tag-link tag-computer-science">computer science</a> <a href="/tag/differential-equation" class="tag-link tag-differential-equation">differential equation</a> <a href="/tag/probability" class="tag-link tag-probability">probability</a> <a href="/tag/stochastic-equation" class="tag-link tag-stochastic-equation">stochastic equation</a> </p><h3>Optimal Stopping Theory and the Secretary Problem</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="https://www.math.upenn.edu/~ted/210F10/References/Secretary.pdf">Thomas S. Ferguson</a></p></div></li><li><div><p><a href="http://theory.stanford.edu/~sergei/slides/hiring-dagstuhl.pdf">Sergei Vassilvitski</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Optimal Stopping Theory and the Secretary Problem">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Optimal Stopping Theory and the Secretary Problem">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a href="/tag/decision-theory" class="tag-link tag-decision-theory">decision theory</a> <a href="/tag/probability" class="tag-link tag-probability">probability</a> <a href="/tag/statistics" class="tag-link tag-statistics">statistics</a> </p><h3>Random Walks and Self-Avoiding Walks</h3><p>Possible reference materials for this topic include</p><ul><li><div><p><a href="http://www.ms.unimelb.edu.au/~guttmann@unimelb/lectures/MAV2003.pdf">Tony Guttmann</a></p></div></li><li><div><p><a href="https://www.math.ubc.ca/~slade/spa_proceedings.pdf">Gordon Slade</a></p></div></li></ul><p>Quick links: <a href="https://www.google.ca/search?q=Random Walks and Self-Avoiding Walks">Google search</a>, <a href="http://search.arxiv.org:8081/?query=Random Walks and Self-Avoiding Walks">arXiv.org search</a>, <a href="/submit-talk">propose to present a talk</a></p><p><a href="/tag/combinatorics" class="tag-link tag-combinatorics">combinatorics</a> <a href="/tag/probability" class="tag-link tag-probability">probability</a> </p></article><footer><p>Help improve this page by <a href="https://github.com/wumss/seminar/edit/master/wiki/tag/Remarkable.Tags.Tag(&quot;probability&quot;).md">editing it on GitHub</a>.</p></footer></section></body></html>
