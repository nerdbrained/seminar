<!DOCTYPE html>
<html><head><meta charset="utf-8" /><meta content="NaytRH7PCHM8SH9XV-xgMLEi_1m1wS9lPBAIvTJ-wvs" name="google-site-verification" /><link rel="stylesheet" href="//yegor256.github.io/tacit/tacit.min.css" /><link rel="stylesheet" href="/css/custom.css" /><script type="text/x-mathjax-config">MathJax.Hub.Config({
    jax: [
        "input/TeX",
        "input/MathML",
        "input/AsciiMath",
        "output/CommonHTML"
    ],
    extensions: [
        "tex2jax.js",
        "mml2jax.js",
        "asciimath2jax.js",
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js"
    ],
    TeX: {
        extensions: [
            "AMSmath.js",
            "AMSsymbols.js",
            "noErrors.js",
            "noUndefined.js"
        ]
    },
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": {
        availableFonts: ["TeX"]
    }
});
</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script><title>Tag Remarkable.Tags.Tag("gradient descent")</title></head><body><nav><ul><li><a href="/"><img src="/images/logo.png" width="64" alt="Home" title="Home" height="64" /></a></li><li><a href="/archive">archive</a></li><li><a href="/tags">tags</a></li><li><a href="/faq">faq</a></li><li><a href="/document">library</a></li><li><a href="/potential-topics">topics</a></li><li><a href="/submit-talk">speak</a></li></ul></nav><section><article><h1>Gradient descent</h1><p>There have been 1 completed talk tagged with <b>gradient descent</b>.</p><h2>Related Tags</h2><ul><li><a href="/tag/machine-learning" class="tag-link tag-machine-learning">machine learning</a></li><li><a href="/tag/optimization" class="tag-link tag-optimization">optimization</a></li></ul><h2>Completed Talks</h2><h3><a href="/archive/rw-Convex">Convex Optimization</a></h3><p>Delivered by Rolina Wu on Friday October 28, 2016</p><p>This talk will introduce the basics for Convex Optimization, several popular optimization algorithms, and the application for convex optimization in Machine Learning.</p><p><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd and Vandenberghe, 2004</a> will be used for reference.</p></article><footer><p>Help improve this page by <a href="https://github.com/wumss/seminar/edit/master/wiki/tag/Remarkable.Tags.Tag(&quot;gradient descent&quot;).md">editing it on GitHub</a>.</p></footer></section></body></html>
