<!DOCTYPE html>
<html><head><meta charset="utf-8" /><meta name="google-site-verification" content="NaytRH7PCHM8SH9XV-xgMLEi_1m1wS9lPBAIvTJ-wvs" /><link href="//yegor256.github.io/tacit/tacit.min.css" rel="stylesheet" /><link href="/css/custom.css" rel="stylesheet" /><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-93527069-1', 'auto');
ga('send', 'pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    jax: [
        "input/TeX",
        "input/MathML",
        "input/AsciiMath",
        "output/CommonHTML"
    ],
    extensions: [
        "tex2jax.js",
        "mml2jax.js",
        "asciimath2jax.js",
        "MathMenu.js",
        "MathZoom.js",
        "AssistiveMML.js"
    ],
    TeX: {
        extensions: [
            "AMSmath.js",
            "AMSsymbols.js",
            "noErrors.js",
            "noUndefined.js"
        ]
    },
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    "HTML-CSS": {
        availableFonts: ["TeX"]
    }
});
</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script><title>Tag Remarkable.Tags.Tag("gradient descent")</title></head><body><nav><ul><li><a href="/"><img src="/images/logo.png" height="64" title="Home" alt="Home" width="64" /></a></li><li><a href="/archive">archive</a></li><li><a href="/tags">tags</a></li><li><a href="/faq">faq</a></li><li><a href="/document">library</a></li><li><a href="/potential-topics">topics</a></li><li><a href="/submit-talk">speak</a></li></ul></nav><section><article><h1>Gradient descent</h1><p>There have been 1 completed talk tagged with <b>gradient descent</b>.</p><h2>Related Tags</h2><ul><li><a class="tag-link tag-machine-learning" href="/tag/machine-learning">machine learning</a></li><li><a class="tag-link tag-optimization" href="/tag/optimization">optimization</a></li></ul><h2>Completed Talks</h2><h3><a href="/archive/rw-Convex">Convex Optimization</a></h3><p>Delivered by Rolina Wu on Friday October 28, 2016</p><p>This talk will introduce the basics for Convex Optimization, several popular optimization algorithms, and the application for convex optimization in Machine Learning.</p><p><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd and Vandenberghe, 2004</a> will be used for reference.</p></article><footer><p>Help improve this page by <a href="https://github.com/wumss/seminar/edit/master/wiki/tag/Remarkable.Tags.Tag(&quot;gradient descent&quot;).md">editing it on GitHub</a>.</p></footer></section></body></html>
